#import "@preview/physica:0.9.8": dv, pdv
#import "meta.typ": conf, ov

#show: conf.with(
  titl: "Лекции по Теории вероятностей",
  desc: "Конспект лекций Рудненко А.В. по Теории вероятностей",
  datet: datetime(year: 2026, month: 1, day: 21),
  head: [Лекции по Теории вероятностей],
)

#outline()
#pagebreak(weak: true)

= Список сокращений

#v(1em)

СС – случайное событие

НСС – несовместные события

СВ – случайная величина

ЗР – закон распределения

ПЗР – параметрический закон распределения

ИФР – интегральная функция распределения

ДФР – дифференциальная функция распределения

ПР – плотность распределения

ПРВ – плотность распределения вероятности

ФР – функция распределения

ПГС – полная группа событий

ПЭС – пространство элементарных событий

ЗС – зависимые события

НЗС – независимые события

ДСВ – дискретная случайная величина

ДСВ – двумерная случайная величина

НСВ – непрерывная случайная величина

ЧХ – числовые характеристики случайных величин

ХР – характеристика рассеяния

ХП – характеристика положения

СКО – среднеквадратичное отклонение

НЗСВ – независимые случайные величины

ФПВ – формула полной вероятности

ТМО – теория массового обслуживания

СМО – система массового обслуживания

ЗБЧ – закон больших чисел

НКСВ – некоррелированные случайные величины

КСВ – коррелированные случайные величины

ЗСВ – зависимые случайные величины

УЗР – условный закон распределения

ФПМО – формула полного математического ожидания

НЗР – нормальный закон распределения

ДНЗР – двумерный нормальный закон распределения

СП – случайные последовательности

ПТТВ – предельная теорема теории относительности

ЗБЧ – закон больших чисел

ЦПТ – центральная предельная теорема

= Случайные события

== Основные понятие теории вероятностей

#v(1em)

Эксперимент (опыт) состоит в том, что проводится испытание при выполнении некоторого комплекса условий, либо создаваемых искусственно, либо осуществляемых не по воле экспериментатора. Должны быть указаны условия и наблюдаемый результат.

Событие - результат (исход) события

Эксперименты - детерменированные или случайные (стохастические)

Случайный эксперимент - эксперимент, проводимый в условиях стохастической однородности (воспроизводимость, непредсказуемость, устойчивость частоты события)

Случайное событие (СС) - событие, которое при осуществлении эксперимента может произойти или не произойти.

== Виды событий

#v(1em)

Пространство элементарных событий $Omega$ - совокупность всех взаимоисключающих событий.

Событие - любое подмножество пространства элементарных событий.

Элементарное событие $omega$ - любой простейший неделимый исход эксперимента.

Совместные - появление одного события не исключают появление другого.

Противоположные события - если в одном и том же эксперименте они несовместны, и одно из них обязательно произойдет.

Достоверное событие - данном эксперименте обязательно произойдет.

Невозможное событие - обязательно не произойдет.

Благоприятное событие - если появление его способствует появлению другого события.

Равные события - содержат одни и те же элементы.

=== Алгебра событий

#v(1em)

Сложение, умножение, разность ($\\$), симметрическая разность ($plus.o$).

Дистрибутивность $ A (B + C) = A B + A C; (A + B) (A + C) $

Формула де Моргана $ ov(A + B) = ov(A) dot ov(B); ov(A B) = ov(A) + ov(B) $

== Вероятность события

=== Классическое определение вероятности (КОВ) и его свойства

#v(1em)

+ Полная группа событий образует совокупность событий $A_1, A_2, ..., A_n => sum_i A_i = Omega; A_i A_j = nothing space forall i != j$
+ События называются равновозможными, если в результате эксперимента появляются одинаково часто

$ P(A) = m/n; P(Omega) = 1; P(nothing) = 0; 0 < P(A) < 1 (A != Omega, A != nothing) => 0 <= P(A) <= 1 $

==== Недостатки КОВ

#v(1em)

Не всегда можно представить результаты как совокупность равновозможных событий; $m$ и $n$ конечные, что не всегда так; слова "равновозможность" и "равновероятность" - синонимы.

=== Геометрическое определение вероятности

#v(1em)

$P(A) = ("mes" q)/("mes" G)$ - мера области, благоприятной появлению события A к мере всей области.

#grid(
  columns: (1fr, 4fr),
  column-gutter: 1em,
  [
    #figure(
      image("source-figures/lect1-3-2.png", width: 60%),
    )
  ],
  [
    #rect[$ (pi (a/2)^2)/a^2 = pi/4 = P(A) $]
  ],
)

=== Статистическое определение вероятности

#v(1em)

Событие А появилось $m$ раз за $n$ опытов, тогда $m$ - абсолютная частота, $W(A) = m/n$ - относительная частота.

#figure(
  table(
    columns: 4,
    stroke: none,
    [$n$], [$m_г$], [$W_г$], [Экспериментатор],
    [4040], [2048], [0,5069], [Бюффон],
    [4092], [2048], [0,5005], [де Морган],
    [10000], [4979], [0,4979], [Феллер],
    [12000], [6019], [0,5016], [Пирсон],
    [20480], [10379], [0,5068], [Джевонс],
    [24000], [12012], [0,5005], [Пирсон],
    [80640], [40151], [0,4979], [Романовский],
  ),
  caption: [Эксперимент с монеткой],
)

$P(A)$ - число, около которого группируется значение частот при большом количестве опытов.

=== Аксиоматическое построение теории вероятностей

#v(1em)

Это осуществили математики Бильберт (1899 год) и Колмогоров (1933 год).

Алгебра событий $FF$ называется любое непустое множество, удовлетворяющее следующим требованиям:

$A in FF => ov(A) in FF \
A, B in FF => A union B in FF, A inter B in FF \
FF_0 = {nothing, Omega} \
FF = {nothing, A, ov(A), Omega}$

==== $sigma$-алгебра ($F$)

#v(1em)

$Omega in F \
A in F => ov(A) in F \
A_1, A_2, ... in F => sum_i A_i in F, product_i A_i in F$

Вероятностной мерой называется числовая функция, заданная в алгебре событий.

$F: A in F; P(A) \
P(A) >= 0 space forall A in F \
P(Omega) = 1 \
P(sum_i A_i) = sum_i P(A_i), A_i A_j = nothing space forall i != j space forall A_i in F \
lim_(n->oo) P(A_n) = 0, A_1 supset A_2 supset ... supset A_n supset ... => product_i A_i = nothing space forall A_i in F$

Конечно аддитивная вероятность, заданная на $sigma$-алгебре называется непрерывной, если для любой убывающей последовательности, имеющей пустое пересечение имеет место равенство $ lim_(n->oo) (P(A_n)) = 0 <= [P(A_n), F]; A_1 supset A_2 supset ... supset A_n supset ... "и" product A_i = nothing $

===== Свойства $sigma$-алгебры

#v(1em)

+ $P(nothing) = 0$
+ $P(ov(A)) = 1 - P(A)$
+ $P(A) <= P(B), A subset B$
+ $0 <= P(A) <= 1$
+ $P(A + B) = P(A) + P(B) - P(A B)$
+ $P(A + B) <= P(A) + P(B)$
+ $P(sum_i A_i) = sum_i P(A_i) - sum_(1<=i<j<=n) P(A_i A_j) + sum_(1<=i<j<m<=n) P(A_i A_j A_m) - ... - (-1)^n P(product_i A_i)$
+ $P(sum_i A_i) <= sum_i P(A_i)$

=== Основные формулы вычисления вероятностей

#v(1em)

Для несовместных событий ($P(A B) = 0$) $ P(A+B) = P(A) + P(B) \ P(sum_i A_i) = sum_i P(A_i) => sum_i P(A_i) = 1, P(A) + P(ov(A)) = 1 "для" sum_i A_i = Omega $

При наличии совместности $ P(A+B) = P(A) + P(B) - P(A B) \ P(sum_i A_i) = sum_i P(A_i) - sum_(1<=i<j<=n) P(A_i A_j) + sum_(1<=i<j<m<=n) P(A_i A_j A_m) - ... - (-1)^n P(product_i A_i) $

==== Пример

#v(1em)

Всего имеется 30 жетонов с номерами от 1 до 30. Событие A - номер делится на 2, событие B - номер делится на 3.

$ P(A + B) = 15/30 + 10/30 - 5/30 = 2/3 $

=== Условная вероятность и независимость события

#v(1em)

Два события называются независимыми, если вероятность появления одного события не зависит от того, появилось ли другое событие или нет.

$ P(B\/A) = P(A B)/P(A), $ при $P(A) = 0$ $P(B\/A)$ не определена.

Условной вероятностью события называется вероятность этого события при условии, что другое произошло.

+ $0 <= P(A\/B) <= 1$
+ $P(ov(A)\/B) = 1 - P(A/B)$
+ $P(A\/A) = 1$
+ $P((A_1 + A_2)\/B) = P(A_1\/B) + P(A_2\/B) - P((A_1 A_2)\/B)$
+ $A B = nothing => P(A\/B) = 0$
+ $A subset B => P(B\/A) = 1$

==== Задача с картами

#v(1em)

$n = 36$ карт \
Событие A - выбранная карта масти пика \
Событие B - выбранная карта - дама

$display(
  cases(
    reverse: #true,
    delim: "|",
    P(A) = 9/36 = 1/4,
    P(B) = 4/36 = 1/9,
    P(A B) = 1/36
  ) => P(B\/A) = (P(A B))/(P(A)) = 1/36 colon 1/4 = 1/9 = P(B) => "A и B - НЗС"
)$

$display(
  cases(
    reverse: #true,
    delim: "|",
    n = 36 + N^(=100),
    P(A) = 9/136,
    P(B) = 4/136 = 1/34,
    P(A B) = 1/136,
  ) => P(B\/A) = (P(A B))/(P(A)) = 1/136 colon 9/136 = 1/9 != P(B) => "A и B - ЗС"
)$

==== Теорема умножения вероятностей

#v(1em)

Для зависимых событий $ P(A B) = P(A) P(B\/A) = P(B) P(A\/B) \ P(A_1 A_2 A_3...A_n) = P(A_1)P(A_2\/A_1)P(A_3\/(A_1 A_2))...P(A_n\/(A_1 A_2...A_(n-1))) $

Для независимых событий $ P(A B) = P(A) P(B), P(A_1 A_2 A_3...A_n) = product_i P(A_i) $

=== Формулы полной вероятности и Байеса

#v(1em)

Полная группа событий, образованная несовместными событиями $ H_1, H_2, ..., H_n, $ то есть $H_i H_j = nothing space forall i != j$ и $sum_i H_i = Omega$.

Следствием теоремы сложения вероятностей для несовместных событий и умножения для независимых событий является формула полной вероятности $ P(A) = sum_i P(H_i)P(A\/H_i) $

==== Задача на полную вероятность

#v(1em)

Всего на зачёте по ТВиМС представлены 50 задач: 30 по теории вероятностей и 20 - по математической статистике. Студент умеет решать 15 задач по теории вероятностей, и 18 - по математической статистике. Для сдачи зачёта требуется решить одну случайную задачу. С какой вероятностью студент сдаст зачёт?

$P(H_1) = 30/50 = "0,6", P(A\/H_1) = 15/30 = "0,5" \
P(H_2) = 20/50 = "0,4", P(A\/H_2) = 18/20 = "0,9" \
P(A) = P(H_1) P(A\/H_1) + P(H_2) P(A\/H_2) = "0,6" dot "0,5" + "0,4" dot "0,9" = "0,66"$

Формула Байеса $ P(H_i\/A) = (P(H_i) P(A\/H_i))/P(A), $ выведено Лапласом

=== Задача на формулу Бернулли

#v(1em)

$display(
  cases(
    delim: #none,
    n = 10,
    m_г = 3,
    m_ц = 7,
  ) quad cases(
    delim: #none,
    P(A) = display(C_10^3/2^10 = C_10^3/tilde(A)_10^2 = 10!/(3! (10 - 3)!) dot (1/2^10)),
    P_n (m) = display(C_n^m p^m q^(n-m) = C_10^3 (1/2)^3 (1/2)^(10-3) = C_10^3/2^10),
  )
)$

= Случайные величины и их распределения

#v(1em)

СВ называется величина, которая в результате эксперимента принимает из множества возможных значений то или иное заранее неизвестное значение.

== Различные примеры получаемых СВ

#v(1em)

+ $X, E = {1, 2, 3, 4, 5, 6}$
+ $X, E = {0, 1/3, 2/3, 1}$
+ $T, E = {t >= 0}$
+ $X, E = {-oo < x < oo}$
+ $X, E = {1, 2, ..., n, ...}$
+ $(X, Y), E = {-oo < x < +oo, -oo < y < +oo}$

$ X = phi(omega), omega in Omega $

== Классификация случайных величин

#v(1em)

ДСВ (Дискретной СВ) называется СВ, которая принимает любое значение из конечного или счетного бесконечного множества возможных значений.

НСВ (Непрерывной СВ) называется СВ, которая принимает любое значение из некоторого промежутка.

Полной характеристикой СВ является закон распределения.

== Закон распределения СВ и способы его задания

#v(1em)

ЗР СВ - правило (соотношение), которое определяет связь между возможными значениями или интервалами возможных значений случайной величины и соответствующими им вероятностями.

#figure(
  table(
    columns: 6,
    [$x$], [$x_1$], [$x_2$], [...], [$x_n$], [...],
    [$p(x = x_i)$], [$p_1$], [$p_2$], [...], [$p_n$], [...],
  ),
  caption: [Таблица закона распределения ДСВ (ряд распределения)],
)

Также ЗР ДСВ может быть задан графиком (@графикДСВ или @прямаяДСВ) или аналитически в виде формулы.

#figure(
  image("source-figures/lect4-fig3.png", width: 70%),
  caption: [],
) <графикДСВ>
#figure(
  image("source-figures/lect4-fig4.png", width: 70%),
  caption: [],
) <прямаяДСВ>

Наиболее общей формой ЗР СВ как для дискретные, так и для непрерывных СВ является функция распределения. Они бывают интегральные (ИФР, иногда И опускают), и дифференциальные (ДФР или ПР - плотность распределения).

ИФР называется вероятность того, что случайная величина $x$ примет значение, меньшее $X$: $ F(x) = P(X < x) $

Для ДСВ $ F(x) = sum_(x_i<x) P(x=x_i) $
Для НСВ $ F(x) = P(X < x) $

=== Свойства функции распределения

#v(1em)

+ $0 <= F(x) <= 1$

+ $F(x_2) >= F(x_1), x_2 >= x_1$

+ $display(lim_(x->-oo)) F(x) = 0$

+ $F(x - 0) = F(x)$ (односторонняя непрерывность)

#grid(
  columns: (2.5fr, 1fr),
  column-gutter: 1em,
  [
    #figure(
      image("source-figures/lect4-fig5.png"),
      caption: [График функции распределения для ДСВ],
    )
  ],
  [#rect[$ F(x) = lim_(epsilon->0) F(x - epsilon) $]],
)

#figure(
  image("source-figures/lect4-fig6.png", width: 70%),
)
#figure(
  image("source-figures/lect4-fig7.png", width: 70%),
  caption: [График функции распределения для НСВ],
)

=== Плотность распределения

#v(1em)

$ f(x) = F'(x) = dv(, x) F(x) $

$f(x) = F'(x) = display(lim_(Delta x -> 0)) (F(x + Delta x) - F(x))/(Delta x)$

==== Свойства ПР

#v(1em)

+ $f(x) >= 0$

+ $integral_(-oo)^(+oo) f(x) dif x = 1$

#grid(
  columns: (3fr, 1fr),
  column-gutter: 1em,
  [
    $P(x_1 < X < x_2) = integral_(x_1)^(x_2) f(x) dif x \
    F(x) = integral_(-oo)^x f(t) dif t$
  ],
  [
    #figure(
      image("source-figures/lect4-fig8.png"),
    )
  ],
)

== Числовые характеристики СВ

#v(1em)

+ Характеристики положения\
  $M(x)$ \
  $M_o (x)$ \
  $M_e (x)$ \
  $nu_1(x) = M(x)$

+ Характеристики рассеяния \
  $D(x)$ \
  $sigma(x)$ - СКО \
  $nu_k (x), k > 1$ \
  $mu_k (x), k > 1; mu_1(x) = 0$ \
  $alpha(x)$ \
  $epsilon(x)$

=== Математическое ожидание

#v(1em)

Для ДСВ $ M(X) = sum_(i=1)^(n (oo)) x_i p_i $
Для НСВ $ M(X) = integral_(-oo)^(+oo) x f(x) dif x $

==== Свойства математического ожидания

#v(1em)

+ $M(C) = C, C = "const"$
+ $M(X+C) = M(X) + C$
+ $M(C X) = C M(X)$
+ $M(C_1 X + C_2) = C_1 M(X) + C_2$
+ $M(X - M(X)) = M(X) - M(M(X)) = 0$
+ $abs(M(X)) <= M(abs(X))$
+ $M(X plus.minus Y) = M(X) plus.minus M(Y); X, Y "- НЗСВ"$
+ $M(X Y) = M(X) M(Y); X, Y "- НЗСВ"$

==== Числовые характеристики математического ожидания

#v(1em)

Модой СВ называется её наиболее вероятное значение $ P(X = M_o (X)) = max_i P(X = x_i) $

#figure(
  image("source-figures/lect5-1.png"),
  caption: [],
) <мода>

На @мода, на графике слева, $M_o = x_3$

Медианой СВ $X$ называется такое число $M_e$, обладающее следующим свойством $ P(X < M_e (X)) = P(X > M_e (X)) = "0,5" $

#grid(
  columns: (1fr, 1fr),
  column-gutter: 1em,
  [
    #figure(
      image("source-figures/lect5-2.png", width: 90%),
    )
  ],
  [
    $display(integral_(-oo)^(M_e (X)) f(x) dif x = integral_(M_e (X))^(+oo) f(x) dif x = "0,5")$
  ],
)

=== Дисперсия

#v(1em)

$"Центрированная СВ" limits(X)^(#circle(radius: 1pt, stroke: 0.4pt)) = X - M(X) \
M(limits(X)^(#circle(radius: 1pt, stroke: 0.4pt))) = M(X - M(X)) = 0$

Дисперсией называется математическое ожидание от квадрата центрированной СВ $ M(X - M(X))^2 = M(X^2 - 2X M(X) + M^2(X)) = M(X^2) - M^2(X) $

Для ДСВ $ D(X) = sum_(i=1)^n (x_i M(X))^2 p_i = sum_(i=1)^n x_i^2 p_i - M^2(X) $
Для НСВ $ D(X) = integral_(-oo)^(+oo) (x - M(X))^2 f(x) dif x = integral_(-oo)^(+oo) x^2 f(x) dif x - M^2(X) $

===== Свойства дисперсии

#v(1em)

+ $D(C) = 0$
+ $D(X + C) = D(X)$
+ $D(C X) = C^2 D(X)$
+ $D(C_1 X + C_2) = C_1^2 D(X)$
+ $D(X plus.minus Y) = D(X) + D(Y); X, Y "- НЗСВ"$
+ $D(X Y) = M(X^2) M(Y^2) - M^2(X) M^2(Y); X, Y "- НЗСВ"$

=== Прочие числовые характеристики

#v(1em)

Среднее квадратическое отклонение (СКО) $ sigma(X) = sqrt(D(X)) $

Коэффициент вариации (КВ) $ V(X) = sigma(X)/M(X) $

Начальный момент k-ого порядка $ nu_k (X) = M(X^k) $
Для ДСВ $ nu_k = sum_(i=1)^n x_i^k p_i $
Для НСВ $ nu_k = integral_(-oo)^(+oo) x^k f(x) dif x $

Центрированная случайная величина (ЦСВ) $ limits(X)^(#circle(radius: 1pt, stroke: 0.4pt)) = X - M(X) $
Нормирование (нормированная СВ) $ limits(X)^* = limits(X)^(#circle(radius: 1pt, stroke: 0.4pt))/sigma(X) $

Центральный момент k-ого порядка $ mu_k (X) = M(limits(X^k)^(#circle(radius: 1pt, stroke: 0.4pt))) = M((M - M(X))^k) $
Для ДСВ $ mu_k = sum_(i=1)^n (x_i - M(X))^k p_i $
Для НСВ $ mu_k = integral_(-oo)^(+oo) (x - M(X))^k f(x) dif x $
$mu_1 (X) = M(X - M(X)) = 0 \
mu_2 (X) = M((X - M(X))^2) = M(X^2 - 2X M(X) + M^2(X)) = M(X^2) - 2M(X) M(X) + \ + M^2(X) = M(X^2) - M^2(X) = nu_2 (X) - nu_1^2 (X)$

#grid(
  columns: (1fr, 1fr),
  column-gutter: 1em,
  [
    Коэффициент асимметрии #rect[$ alpha(X) = (mu_3 (X))/(sigma^3 (X)) $]
  ],
  [
    #figure(
      image("source-figures/lect5-3.png", width: 90%),
    )
  ],
)

#grid(
  columns: (2fr, 1fr),
  column-gutter: 1em,
  [
    Коэффициент скошенности, иначе называется островершинностью #rect[$ epsilon(X) = (mu_4 (X))/(sigma^4 (X)) - 3 $]
  ],
  [
    #figure(
      image("source-figures/lect5-4.png"),
    )
  ],
)

=== Квантили и процентные точки

#v(1em)

Квантилем $q$ ($0 < q < 1$) называется такое значение $x_q$, при котором функция её распределения принимает значение, равное $q$

$ F(x_q) = P(X < x_q) = q $

$x_"0,5" = M_e (X) \
x_"0,1", x_"0,2", ... "- децили" \
x_"0,01", x_"0,02", ... \
x_"0,25", x_"0,5", x_"0,75" "- квартили" \
x_(1-q): P(X >= x_(1-q)) = q$

$display(
  f(x) = cases(
    0\, &x < 0,
    3x^2\, &0 <= x <= 1,
    0\, &x > 1,
  ) => F(x) = cases(
    0\, &x < 0,
    x^3\, &0 <= x <= 1,
    1\, &x > 1,
  ) \
  x < 0: F(x) = integral_(-oo)^x 0 dif t = 0 \
  0 <= x <= 1: F(x) = integral_(-oo)^0 0 dif t + integral_0^x 3t^2 dif t = x^3 \
  x > 1: F(x) = integral_(-oo)^0 0 dif t + integral_0^1 3t^2 dif t + integral_1^(+oo) 0 dif t = 1 \
  x_"0,3": F(x_"0,3") = x_"0,3"^3 = "0,3" => x_"0,3" = root(3, "0,3") approx "0,67" \
  "30%": F(x_(1-"0,3")) = x_(1-"0,3")^3 = "0,7" => x_(1-"0,3") = root(3, "0,7") approx "0,89"
)$

== Шаблон, по которому изучаются основные законы распределения СВ

#v(1em)

+ Определение
+ Краткая форма записи
+ Закон распределения (ЗР) (либо для ДСВ: ряд, многоугольник, функция распределения, либо для НСВ: плотность распределения, функция распределения).
+ Числовые характеристики
+ Вероятность попадания
+ Область применения
+ Задача
+ Прочее
+ Домашнее задание

=== Биномиальный ЗР

#v(1em)

#grid(
  columns: (3fr, 1fr),
  column-gutter: 1em,
  [
    1. Для ДСВ $X$ задаются параметры $n$ и $p$: \
      $0 < p < 1, q = 1 - p => 0 < q < 1; 0 <= m <= n$

      $X = 0, 1, 2, ..., m, ..., n$
  ],
  [
    #figure(
      image("source-figures/lect6-1.png"),
    )
  ],
)
$ P_n (m) = C_n^m p^m q^(n-m) $

2. $X ~ "Bi"(n, p)$

3. #table(
    columns: 8,
    align: center,
    [$x_i$], [0], [1], [2], [...], [$m$], [...], [$n$],
    [$p_i$], [$q^n$], [$C_n^1 p q^(n-1)$], [$C_n^2 p^2 q^(n-2)$], [...], [$C_n^m p^m q^(n-m)$], [...], [$p^n$],
  )

4. $n p - q <= M_o (X) <= n p + p$ \
  $M(X) = n p$ \
  $D(X) = n p q$ \
  $sigma(X) = sqrt(n p q)$ \
  $alpha(X) = (q - p)/sqrt(n p q)$ \
  $epsilon(X) = (1 - 6p q)/(n p q)$

5. $P(x_1 < X < x_2) = sum_i P(X=m_i)$

  $m/n: &M(m/n) = p \
  &D(m/n) = (p q)/n$

6. ...
7. Две фабрики производят обувь в соотношении 2:3. Было куплено 4 пары обуви. Найти вероятность того, что 2 или 3 пары были произведены на первой фабрике \
  $X ~ "Bi"(4; 0,4)$ \
  #table(
    columns: 6,
    align: center,
    [$x_i$], [0], [1], [2], [3], [4],
    [$p_i$], [0,1296], [0,3456], [0,3456], [0,1536], [0,0256],
  ) \
  $M_o_1 (X) = 1, M_o_2 (X) = 2$

  $M(X) = "1,6"; D(X) = "0,96"; sigma(X) approx "0,98"; alpha(X) = "0,2"/"0,98" approx "0,204"; epsilon(X) approx "-0,458"$

  $P(2 <= X <= 3) = P(X = 2) + P(X = 3) = "0,4992"$

=== Закон распределения Пуассона

#v(1em)

1. Для ДСВ $X$ задаётся положительный параметр $lambda$; $lambda = n p, n >= 10 "или" n >= 100, n p < 10$ \
  $X = 0, 1, 2, ..., m, ...$

$ P(X=m) = (lambda^m e^(-lambda))/m! $

2. $X ~ "Po"(lambda) "или" X ~ Pi(lambda)$
3. #table(
    columns: 7,
    align: center + horizon,
    [$x_i$], [0], [1], [2], [...], [$m$], [...],
    [$p_i$],
    [$e^(-lambda)$],
    [$lambda e^(-lambda)$],
    [$ (lambda^2 e^(-lambda))/2! $],
    [...],
    [$ (lambda^m e^(-lambda))/m! $],
    [...],
  )
4. $M(X) = n p space (lambda)$

  $D(X) = n p$

  $sigma(X) = sqrt(n p)$

  $alpha(X) = 1/sqrt(n p)$

  $epsilon(X) = 1/(n p)$

5. $P(x_1 < X < x_2) = sum_i P(X=m_i)$
6. ...
7. 1000 веретен работает на ткацкой фабрике, $p = "0,002"$, $P(X > 3) "- ?"$ \
  $n = 1000, n p = 2$

  $P(X > 3) = 1 - P(X <= 3) = 1 - ((e^(-2) dot 2^0)/0! + (e^(-2) dot 2)/1! + (e^(-2) dot 2^2)/2! + (e^(-2) dot 2^3)/3!) = 1 - e^(-2) times \ times (1 + 2 + 4/2 + 8/6) approx "0,143"$

=== Геометрический ЗР (распределение Фарри)

#v(1em)

1. Для ДСВ $X$ задаётся параметр $p$ - вероятность успеха \
  $X = 1, 2, ..., m, ...$

$ P(X=m) = p q^(m-1) $

2. $X ~ "G"(p)$
3. #table(
    columns: 7,
    [$x_i$], [1], [2], [3], [...], [$m$], [...],
    [$p_i$], [$p$], [$p q$], [$p q^2$], [...], [$p q^(m-1)$], [...],
  )

4. $M(X) = 1/p$

  $D(X) = q/p^2$

  $sigma(X) = sqrt(q)/p$

  $alpha(X) = (2-p)/sqrt(q)$

  $epsilon(X) = 6 + p^2/q$

5. $P(x_1 < X < x_2) = sum_i P(X=m_i)$
6. Стрелок совершает большое количество выстрелов, соответственно результаты опыта - попал или не попал.
7. Большая партия товаров, вероятность брака 0,1 \
  #table(
    columns: 7,
    [$x_i$], [1], [2], [3], [4], [...], [$m$],
    [$p_i$], [0,1], [0,09], [0,081], [0,0729], [...], [$"0,9"^m dot "0,1"$],
  )

=== Закон распределения Паскаля

#v(1em)

Для ДСВ задаются параметры $k$ - изначальное число неудач. ЗР Фарри - это ЗР Паскаля при $k = 1$.

$ P(X=m) = C_(m-1)^(k-1) p^k q^(m-k), $ $m = k, k+1, ...$

$M(X) = k/p \
D(X) = (k q)/p^2$

=== Отрицательное биномиальное распределение

#v(1em)

По-хорошему это то же, что и ЗР Паскаля, но почему-то тот был дан своим определением.

$ P(X=m) = C_(m+k-1)^m p^k q^m, $ $m = 0, 1, 2, ...$

$M(X) = (k q)/p \
D(X) = (k q)/p^2$

#table(
  columns: 7,
  align: center,
  [$x_i$], [3], [4], [5], [...], [$m$], [...],
  [$p_i$], [0,001], [0,0027], [0,00486], [...], [$"0,1"^3 C_(m-1)^2 "0,9"^(m-3)$], [...],
)

==== Задача на ЗР Фарри

#v(1em)

Стрельба до первого попадания, $p = "0,2"$, $M(X) "- ?", D(X) "- ?"$

$X "- количество выстрелов"$

1) $n -> oo: M(X) = 1/"0,2" = 5, D(X) = "0,8"/"0,04" = 20$

2) $n = 5:$

#table(
  columns: 7,
  [$x_i$], [1], [2], [3], [4], [5], [...],
  [$p_i$], [$p$], [$q p$], [$q^2 p$], [$q^3 p$], [$q^4 p$], [...],
)

$P(X=5) = q^4 p + q^5 = q^4 (p + q) = q^5$ - берётся уже как биномиальное распределение

=== Закон распределения "для эрудиции"

#v(1em)

Я даже не запомнил его название...

Для ДСВ $X$ задаются параметры $n$, $M$ и $N$

$X = 0, 1, 2, ..., m, ...$ (берётся вплоть до значения $min(n, M)$)

$M <= N$; $n <= N$; $n, M, N in NN$, при этом $N -> oo$, $M -> oo$, $M/N -> p$

$M(X) = n dot M/N \
D(X) = n dot M/(N-1) (1 - M/N) (1 - n/N)$

==== Пример задачи на данный ЗР

#v(1em)

Спортлото 6 из 45

$n = 6, M = 6, N = 45$

$P(3 <= X <= 6) = ...$

=== Равномерный закон распределения

#v(1em)

#grid(
  columns: (1.5fr, 1fr),
  column-gutter: 1em,
  [
    Для НСВ $X$ задаются параметры $a$, $b$, являющиеся границами отрезка $[a, b]$
    #rect[$ X ~ "R"(a, b) $]
    $display(
      f(x) = cases(
        1/(b-a)\, &a <= x <= b,
        0\, &x < a "или" x > b,
      ) \
      F(x) = cases(
        0\, &x <= a,
        (x-a)/(b-a)\, &a < x <= b,
        1\, &x > b,
      )
    )$
  ],
  [
    #figure(
      image("source-figures/lect7-1.png"),
    )
  ],
)

$ P(x_1 < X < x_2) = integral_(x_1)^(x_2) f(x) dif x = F(x_2) - F(x_1) $

Мода при таком ЗР не определена.

$M_e (X) = (a+b)/2 \
M(X) = (a+b)/2 \
D(X) = (b-a)^2/12 \
sigma(X) = (b-a)/(2sqrt(3)) \
alpha(X) = 0 \
epsilon(X) = "-1,2"$

==== Пример равномерного ЗР

#v(1em)

#grid(
  columns: (1fr, 1.5fr),
  column-gutter: 1em,
  [
    #figure(
      image("source-figures/lect7-2.png"),
    )
  ],
  [
    $M_e (X) = 1 \
    M(X) = 1 \
    D(X) = 1/3 \
    sigma(X) = sqrt(3)/3 \
    alpha(X) = 0 \
    epsilon(X) = "-1,2" \
    P(X < "0,5") = integral_0^"0,5" "0,5" dif x = "0,25" = F("0,5") - F(0) = S_(rect.filled)/S$
  ],
)

=== Экспоненциальный ЗР

#v(1em)

#grid(
  columns: (1.5fr, 1fr),
  column-gutter: 1em,
  [
    Для НСВ $X$ задаётся положительный параметр $lambda$
    #rect[$ X ~ E(lambda) $]
    $display(
      f(x) = cases(
        lambda e^(-lambda x)\, &x >= 0,
        0\, &x < 0,
      ) \
      F(x) = cases(
        1 - e^(-lambda x)\, &x >= 0,
        0\, &x < 0
      )
    ) \
    M_o (X) = 0 \
    M_e (X) = (ln 2)/(lambda) \
    M(X) = 1/(lambda) \
    D(X) = 1/(lambda^2) \
    sigma(X) = 1/(lambda) \
    alpha(X) = 2 \
    epsilon(X) = 6$
  ],
  [
    #figure(
      image("source-figures/lect7-3.png"),
    )
  ],
)

==== Задача на экспоненциальный ЗР

#v(1em)

В среднем требуется 15 часов на ремонт станка

$M(X) = 15 "ч" => lambda = 1/15 \
P(X >= 20 "ч") = 1 - P(X < 20) = 1 - P(0 <= X < 20) = 1 - (F(20) - F(0)) = \ = 1 - ((1 - e^(-20/15)) - (1 - e^0)) = e^(-4/3) approx "0,264"$

=== Закон распределения Вейбулла ("для эрудиции")

#grid(
  columns: (1fr, 1fr),
  column-gutter: 1em,
  [
    Для НСВ $X$ задаются положительные параметры $alpha$ и $lambda$
    #rect[$ X ~ "B"(alpha, lambda) $]
    $f(x) = alpha lambda e^(alpha - 1) e^(-lambda x^(alpha)), x >= 0 \
    F(x) = 1 - e^(-lambda x^(alpha))$

    На графике (@вейбулла) $lambda(t) = (f(t))/(1 - F(t))$; при $t < t_1$ проходит приработка, при $t_1 < t < t_2$ - нормальная эксплуатация, при $t > t_2$ - старение (износ).
  ],
  [
    #figure(
      image("source-figures/lect7-4.png"),
      caption: [],
    ) <вейбулла>
  ],
)

=== Нормальный закон распределения

#v(1em)

#grid(
  columns: (1.5fr, 1fr),
  column-gutter: 1em,
  [
    Для НСВ $X$ задаются параметры $a$ - математическое ожидание (среднее значение) и $sigma$ - среднее квадратическое отклонение.
    #rect[$ X ~ "N"(a, sigma^2) $]
    $f(x) = display(1/(sigma sqrt(2pi)) e^(-(x-a)^2/(2sigma^2)))
    F(x) = display(1/(sigma sqrt(2pi)) integral_(-oo)^x e^(-(t-a)^2/(2sigma^2)) dif t)$

    Нормирование и центрирование данной СВ осуществляется при $a = 1$ и $sigma = 0$, соответственно

    $limits(X)^* = display((x - M(X))/sigma(X)) ~ N(0, 1)$

    $M_o (X) = a \
    M_e (X) = a \
    M(X) = a \
    D(X) = sigma^2 \
    sigma(X) = sigma \
    alpha(X) = 0 \
    epsilon(X) = 0$
  ],
  [
    #figure(
      image("source-figures/lect7-5.png"),
    )
  ],
)

Для вычисления вероятности используется функция Лапласа $Phi(X)$, аргументом которого является нормированная и центрированная СВ $X$.

#rect[$ P(x_1 < X < x_2) = 1/2 Phi((x_2 - M(X))/sigma(X)) - 1/2 Phi((x_1 - M(X))/sigma(X)) $]
#rect[$ P(abs(x - a) < epsilon) = 2Phi((epsilon)/(sigma)) $]

==== Правило трёх сигм

#v(1em)

$epsilon = sigma, P = 2Phi(1) approx "0,6827" \
epsilon = 2sigma, P = 2Phi(2) approx "0,9545" \
epsilon = 3sigma, P = 2Phi(3) approx "0,9973"$

==== Задача на нормальный ЗР

#v(1em)

Рост людей имеет распределение $X ~ "N"(173, 36)$

+ $f(x) = display(1/(6sqrt(2pi)) e^(-(x-173)^2/(2 dot 36)))$

+ $F(x) = display(1/(6sqrt(2pi)) integral_(-oo)^x e^(-(t-173)^2/(2 dot 36)) dif t = "0,5" + 1/2 Phi((x - 173)/6))$

+ $176 < X < 182: P(176 < X < 182) = Phi((182-173)/6) - Phi((176-173)/6)$

+ $170 < X < 176: P(170 < X < 176) = Phi((176-173)/6) - Phi((170-173)/6)$

+ $x_"0,7": F(x_"0,7") = 1 - P(X >= x_"0,7") = 1 - P(x_"0,7" <= X < +oo) = \ = 1 - (Phi((+oo - 173)/6) - Phi((x_"0,7" - 173)/6)) = "0,7" <=> Phi((x_"0,7" - 173)/6) = "0,2" <=> Phi(t) = "0,2" <=> \ <=> t = "0,525" <=> x_"0,7" = 6t + 173 = 6 dot "0,525" + 173 approx 176$

=== Логнормальное распределение

#v(1em)

#grid(
  columns: (1.5fr, 1fr),
  column-gutter: 1em,
  [
    $x > 0, X < x => ln X < ln x \
    F(x) = P(X < x) = P(ln X < ln x) = display(1/(sigma sqrt(2pi)) integral_(-oo)^(ln x) e^(-(t - ln a)^2/(2sigma^2)) dif t) \
    f(x) = display(1/(sigma sqrt(2pi)) e^(-(ln x - ln a)^2/(2sigma^2))) \
    M(X) = a e^(sigma^2/2) \
    D(X) = a^2 e^sigma^2 (e^sigma^2 - 1) \
    M_o (X) = a e^(-sigma^2) \
    M_e (X) = a$
  ],
  [
    #figure(
      image("source-figures/lect8-1.png"),
    )
  ],
)

== Многомерные случайные величины

#v(1em)

Совокупность СВ $(X_1, X_2, ..., X_n)$ называют многомерной (векторной), каждая составляющая называется компонентой.

$ (X_1, X_2, ..., X_n) = f(x) $

Дискретной многомерной СВ называется многомерная СВ, компоненты которой дискретны.

=== Законы распределения двумерной случайной величины

#v(1em)

$ F(x, y) = P(X < x, Y < y) $
Для ДСВ $ F(x, y) = sum_(x_i < x) sum_(y_i < y) p_(i j) $

+ $0 <= F(x, y) <= 1$

+ $F(x_2, y) > F(x_1, y), x_2 > x_1$ \
  $F(x, y_2) > F(x, y_1), y_2 > y_1$

+ $F(-oo, y) = 0, F(x, -oo) = 0, F(-oo, -oo) = 0$ \
  $F(+oo, +oo) = 0$

+ $F(x, oo) = F_1(x), F(oo, y) = F_2(y)$

#block(breakable: false)[#grid(
  columns: (1fr, 1.5fr),
  column-gutter: 1em,
  [
    #table(
      columns: 7,
      align: center + horizon,
      [$X\/Y$], [$y_1$], [$y_2$], [...], [$y_j$], [...], [$y_m$],
      [$x_1$], [$p_11$], [$p_12$], [...], [$p_(1 j)$], [...], [$p_(1 m)$],
      [$x_2$], [$p_21$], [$p_22$], [...], [$p_(2 j)$], [...], [$p_(2 m)$],
      [...], [...], [...], [...], [...], [...], [...],
      [$x_i$], [$p_(i 1)$], [$p_(i 2)$], [...], [$p_(i j)$], [...], [$p_(i m)$],
      [...], [...], [...], [...], [...], [...], [...],
      [$x_n$], [$p_(n 1)$], [$p_(n 2)$], [...], [$p_(n j)$], [...], [$p_(n m)$],
    )
  ],
  [
    #rect[$ sum_(i=1)^n sum_(j=1)^m p_(i j) = 1 $]
    $P(x_1 < X < x_2, Y < y) = F(x_2, y) - F(x_1, y) \
    P(X < x, y_1 <= Y <= y_2) = F(x, y_2) - F(x, y_1) \
    (X, Y) in D \
    P(x_1 <= X <= x_2, y_1 <= Y <= y_2) = \ = F(x_2, y_2) - F(x_1, y_2) - F(x_2, y_1) + F(x_1, y_1)$
  ],
)]

Плотностью распределения (дифференциальной функцией) НДСВ называют вторую смешанную производную.

$ f(x, y) = pdv(F(x,y), x, y), $ $F(x, y) = integral_(-oo)^x integral_(-oo)^y f(t, l) dif t dif l$

#grid(
  columns: (3fr, 1fr),
  column-gutter: 1em,
  [
    + $f(x, y) >= 0$

    + $integral_(-oo)^(+oo) integral_(-oo)^(+oo) f(x, y) dif x dif y = 1$

    #rect[$ P(x_1 < X < x_2, y_1 < Y < y_2) = integral_(x_1)^(x_2) integral_(y_1)^(y_2) f(x, y) dif x dif y $]
  ],
  [
    #figure(
      image("source-figures/lect8-2.png", width: 90%),
    )
  ],
)

$ nu_(k,s) (X, Y) = M(X^k Y^s) $
Для ДСВ $ nu_(k,s) = sum_(i=1)^n sum_(j=1)^m x_i^k y_j^s p_(i j) $
Для НСВ $ nu_(k,s) = integral_(-oo)^(+oo) integral_(-oo)^(+oo) x_i^k y_j^s f(x, y) dif x dif y $

$nu_(1,0) (X, Y) = M(X Y^0) = M(X) \
nu_(0,1) (X, Y) = M(X^0 Y) = M(Y) \
nu_(2,0) (X, Y) = M(X^2 Y^0) = M(X^2) = nu_2 (X) \
nu_(0,2) (X, Y) = M(X^0 Y^2) = M(Y^2) = nu_2 (Y) \
nu_(1,1) (X, Y) = M(X Y)$

$ mu_(k,s) = M((X - M(X))^k (Y - M(Y))^s) $
Для ДСВ $ mu_(k,s) = sum_(i=1)^n sum_(j=1)^m (x_i - M(X))^k (y_j - M(Y))^s p_(i j) $
Для НСВ $ mu_(k,s) = integral_(-oo)^(+oo) integral_(-oo)^(+oo) (x_i - M(X))^k (y_j - M(Y))^s f(x, y) dif x dif y $

$mu_(1,0) (X, Y) = M((X - M(X)) (Y - M(Y))^0) = 0 \
mu_(0,1) (X, Y) = M((X - M(X))^0 (Y - M(Y))) = 0 \
mu_(2,0) (X, Y) = M((X - M(X))^2 (Y - M(Y))^0) = D(X) \
mu_(0,2) (X, Y) = M((X - M(X))^0 (Y - M(Y))^2) = D(Y) \
mu_(1,1) (X, Y) = M((X - M(X)) (Y - M(Y)))$

=== Ковариация двумерных СВ

#v(1em)

$ k(X, Y) equiv "cov"(X, Y) = mu_(1,1) (X, Y) = M((X - M(X)) (Y - M(Y))) = M(X Y) - M(X) M(Y) $
Для ДСВ $ "cov"(X, Y) = sum_(i=1)^n sum_(j=1)^m (x_i - M(X)) (y_j - M(Y)) p_(i j) = sum_(i=1)^n sum_(j=1)^m x_i y_j p_(i j) - M(X) M(Y) $
Для НСВ $ "cov"(X, Y) = integral_(-oo)^(+oo) integral_(-oo)^(+oo) (x_i - M(X)) (y_j - M(Y)) f(x, y) dif x dif y $

+ $"cov"(X, Y) = 0; X, Y "- НЗСВ"$
+ $"cov"(X, Y) = "cov"(Y, X)$
+ $"cov"(X, X) = D(X)$
+ $"cov"(Y, Y) = D(Y)$
+ $abs("cov"(X, Y)) <= sqrt(D(X) D(Y))$
+ $"cov"(alpha_1 + beta_1 X, alpha_2 + beta_2 Y) = beta_1 beta_2 "cov"(X, Y)$
+ $"cov"(X + alpha, Y + beta) = "cov"(Y, X)$
+ $"cov"(alpha X + beta Y, Z) = alpha"cov"(X, Z) + beta"cov"(Y, Z)$

Ковариационная матрица $ K = mat(k(X, X), k(X, Y); k(Y, X), k(Y, Y)) = mat(D(X), k(X, Y); k(Y, X), D(Y)) $

=== Корреляция двумерных СВ

#v(1em)

$ r(X, Y) equiv "cor"(X, Y) = ("cov"(X, Y))/(sigma(X) sigma(Y)) $

#grid(
  columns: (2fr, 1fr),
  column-gutter: 1em,
  [
    $abs(r(X, Y)) <= 1 \
    r(X, Y) = 0 "если" X, Y "- НКСВ или" X, Y "- НЗСВ" \
    r(X, Y) = 1 "- функциональная зависимость" \
    r(X, Y) = 1 => Y = alpha + beta X, beta > 0 \
    r(X, Y) = -1 => Y = alpha + beta X, beta < 0 \
    r(alpha_1 + beta_1 X, alpha_2 + beta_2 Y) = r(Y, X), beta_1 beta_2 > 0 \
    r(alpha_1 + beta_1 X, alpha_2 + beta_2 Y) = -r(Y, X), beta_1 beta_2 < 0$

    Корреляционная матрица

    $display(R = mat(r(X, X), r(X, Y); r(Y, X), r(Y, Y)) = mat(1, r(X, Y); r(Y, X), 1))$

    #table(
      columns: 2,
      stroke: none,
      table.header(align(center)[*Степень связи*], align(center)[*$r$*]),
      [слабая], [0,1-0,3],
      [умеренная (средняя)], [0,3-0,5],
      [заметная (сильная)], [0,5-0,7],
      [высокая], [0,7-0,9],
      [весьма (очень) высокая], [0,9-0,99],
    )
  ],
  [
    #figure(
      image("source-figures/lect8-3.png"),
    )
  ],
)

=== ЗР и числовые характеристики компонент двумерной СВ

==== Законы распределения

#v(1em)

$
  p_i equiv P(X=x_i) = sum_(j=1)^m p_(i j) \
  p_j equiv P(Y=y_j) = sum_(i=1)^n p_(i j)
$

#grid(
  columns: (1fr, 1fr),
  column-gutter: 1em,
  [
    #table(
      columns: 4,
      align: center,
      [$X\/Y$], [$y_1$], [$y_2$], [$p_i$],
      [$x_1$], [0,1], [0,06], [0,16],
      [$x_2$], [0,3], [0,18], [0,48],
      [$x_3$], [0,2], [0,16], [0,36],
      [$p_j$], [0,6], [0,4], [$sum p_(i j)=1$],
    )
  ],
  [
    #table(
      columns: 4,
      stroke: none,
      align: center,
      [$X$], [$x_1$], [$x_2$], [$x_3$],
      [$p_i$], [0,16], [0,48], [0,36],
    )
    #table(
      columns: 3,
      stroke: none,
      align: center,
      [$Y$], [$y_1$], [$y_2$],
      [$p_j$], [0,6], [0,4],
    )
  ],
)

$
  F_1(x) = P((X, Y): X < x) = lim_(y->+oo) F(x, y) = F(x, +oo) \
  F_2(y) = P((X, Y): Y < y) = lim_(x->+oo) F(x, y) = F(+oo, y)
$

$display(
  F_1(x) = integral_(-oo)^x integral_(-oo)^(+oo) f(x, y) dif x dif y\, f_1(x) = dv(F_1(x), x) = integral_(-oo)^(+oo) f(x, y) dif y \
  F_2(y) = integral_(-oo)^(+oo) integral_(-oo)^y f(x, y) dif x dif y\, f_2(y) = dv(F_2(y), y) = integral_(-oo)^(+oo) f(x, y) dif x
)$

==== Числовые характеристики

#v(1em)

Для ДСВ
$
  M(X) = sum_(i=1)^n sum_(j=1)^m x_i p_(i j) \
  M(Y) = sum_(i=1)^n sum_(j=1)^m y_j p_(i j)
$

Для НСВ
$
  M(X) = integral_(-oo)^(+oo) integral_(-oo)^(+oo) x f(x, y) dif x dif y = integral_(-oo)^(+oo) x f_1(x) dif x \
  M(Y) = integral_(-oo)^(+oo) integral_(-oo)^(+oo) y f(x, y) dif x dif y = integral_(-oo)^(+oo) y f_2(y) dif y
$

===== Законы сложения и умножения мат. ожидания и дисперсии

#v(1em)

$&M(X + Y) = M(X) + M(Y) \
&M(X Y) = M(X) M(Y) + k(X, Y); &X, Y& "- КСВ" \
&M(X Y) = M(X) M(Y); &X, Y& "- НЗСВ" \
&D(X + Y) = D(X) + D(Y) + 2k(X, Y); &X, Y& "- КСВ" \
&D(X + Y) = D(X) + D(Y) - 2k(X, Y); &X, Y& "- КСВ" \
&D(X plus.minus Y) = D(X) + D(Y); &X, Y& "- НЗСВ"$

==== Про зависимость и коррелированность СВ

#v(1em)

2 СВ называются зависимыми, если ЗР одной из них зависит от того, какое значение приняла другая СВ.

Коррелированными называются 2 СВ, если $k(X, Y) != 0$ ($r(X, Y) != 0$).

НЗСВ $=>$ НКСВ; НКСВ $arrow.double.not$ НЗСВ; КСВ $=>$ ЗСВ; ЗСВ $=>$ КСВ.

==== Условные ЗР (УЗР) компонент двумерной СВ

#v(1em)

УЗР одной компоненты двумерной СВ называется её ЗР при условии, что другая компонента приняла определенное значение.

$
  P(x_i \/ y_j) = P(X=x_i \/ Y=y_j) = P(x_i, y_j)/P(y_j), i=1...n, sum_(i=1)^n P(x_i \/ y_j) = 1 \
  P(y_j \/ x_i) = P(Y=y_j \/ X=x_i) = P(x_i, y_j)/P(x_i), j=1...n, sum_(i=1)^n P(y_j \/ x_i) = 1
$

#grid(
  columns: (1fr, 1fr),
  column-gutter: 1em,
  [
    #table(
      columns: 4,
      align: center,
      [$X\/Y$], [$y_1$], [$y_2$], [$p_i$],
      [$x_1$], [0,1], [0,06], [0,16],
      [$x_2$], [0,3], [0,18], [0,48],
      [$x_3$], [0,2], [0,16], [0,36],
      [$p_j$], [0,6], [0,4], [$sum p_(i j)=1$],
    )
  ],
  [
    #table(
      columns: 4,
      stroke: none,
      align: center,
      [$X$], [$x_1$], [$x_2$], [$x_3$],
      [$p_i$], [0,16], [0,48], [0,36],
    )
    #table(
      columns: 3,
      stroke: none,
      align: center,
      [$Y$], [$y_1$], [$y_2$],
      [$p_j$], [0,6], [0,4],
    )
  ],
)

#table(
  columns: 5,
  stroke: none,
  align: center,
  [$x_i$], [$x_1$], [$x_2$], [$x_3$], [$sum$],
  [$P(x_i\/y_1)$], [$"0,1"/"0,6"=1/6$], [$"0,3"/"0,6"=1/2$], [$"0,2"/"0,6"=1/3$], [1],
  [$P(x_i\/y_2)$], [$"0,06"/"0,4"=3/20$], [$"0,18"/"0,4"=9/20$], [$"0,16"/"0,4"=2/5$], [1],
)
#table(
  columns: 4,
  stroke: none,
  align: center,
  [$y_j$], [$y_1$], [$y_2$], [$sum$],
  [$P(y_j\/x_1)$], [$"0,1"/"0,16"=5/8$], [$"0,06"/"0,16"=3/8$], [1],
  [$P(y_j\/x_2)$], [$"0,3"/"0,48"=5/8$], [$"0,18"/"0,48"=3/8$], [1],
  [$P(y_j\/x_3)$], [$"0,2"/"0,36"=5/9$], [$"0,16"/"0,36"=4/9$], [1],
)

$F(x, y) = F_1(x) F_2(y\/x) => F_2(y\/x) = F(x, y)/(F_1(x)) \
F(x, y) = F_2(y) F_1(x\/y) => F_1(x\/y) = F(x, y)/(F_2(y)) \
f(x, y) = f_1(x) f_2(y\/x) => f_2(y\/x) = f(x, y)/(f_1(x)) \
f(x, y) = f_2(y) f_1(x\/y) => f_1(x\/y) = f(x, y)/(f_2(y))$

===== Теорема умножения функций распределения

#v(1em)

Функцию распределения двумерной СВ можно представить как произведение $F(x, +oo) = F_1(x)$ на функцию распределения $Y$ при условии $x$.

===== Теорема о независимости случайных величин

#v(1em)

$
  & P(x_1 < X < x_2, y_1 < Y < y_2) = P(x_1 < X < x_2) P(y_1 < Y < y_2) \
  & F(x, y) = F_1(x) F_2(y) \
  & f(x, y) = f_1(x) f_2(y)
$

==== Условные числовые характеристики двумерной случайной величины

#v(1em)

Для ДСВ
$
  M(X\/y_j) = sum_(i=1)^n x_i P(x_i\/y_j) = 1/p(y_j) sum_(i=1)^n x_i P(x_i, y_j) \
  M(Y\/x_i) = sum_(j=1)^n y_j P(y_j\/x_i) = 1/p(x_i) sum_(j=1)^n y_j P(x_i, y_j) \
  D(X\/y_j) = sum_(i=1)^n (x_i - M(X\/y_j))^2 P(x_i\/y_j) = 1/p(y_j) sum_(i=1)^n (x_i - M(X\/y_j))^2 P(x_i, y_j) \
  D(Y\/x_i) = sum_(j=1)^n (y_j - M(Y\/x_i))^2 P(y_j\/x_i) = 1/p(x_i) sum_(j=1)^n (y_j - M(Y\/x_i))^2 P(x_i, y_j)
$

Для НСВ
$
  M(X\/y) = integral_(-oo)^(+oo) x f_1(x\/y) dif x = 1/(f_2(y)) integral_(-oo)^(+oo) x f(x, y) dif x \
  M(Y\/x) = integral_(-oo)^(+oo) y f_2(y\/x) dif y = 1/(f_1(x)) integral_(-oo)^(+oo) y f(x, y) dif y \
  D(X\/y) = integral_(-oo)^(+oo) (x - M(X\/y))^2 f_1(x\/y) dif x = 1/(f_2(y)) integral_(-oo)^(+oo) (x - M(X\/y))^2 f(x, y) dif x \
  D(Y\/x) = integral_(-oo)^(+oo) (y - M(Y\/x))^2 f_2(y\/x) dif y = 1/(f_1(x)) integral_(-oo)^(+oo) (y - M(Y\/x))^2 f(x, y) dif y
$

===== Уравнение линейной регрессии

#v(1em)

Регрессией называется зависимость условного математического ожидания одной компоненты двумерной СВ от другой.

#figure(
  image("source-figures/lect9-1.png"),
)

$Y$ на $X$: $ y = M(Y) + r(X, Y) sigma(Y)/sigma(X) (x - M(X)) $
$X$ на $Y$: $ x = M(X) + r(X, Y) sigma(X)/sigma(Y) (y - M(Y)) $

==== Формула полного математического ожидания

#v(1em)

$
  M(X) = M(M(X\/Y)) \
  M(Y) = M(M(Y\/X))
$

=== Плотность распределения для двумерных распределений

#v(1em)

$(X, Y): f(x, y) = "const", f(x, y) = display(
  cases(
    1/S_D\, &(x, y) in D,
    0\, &(x, y) in.not D,
  )
) \
P((X, Y) in D_1) = S_D_1/S_D, D_1 subset D$

==== Равномерное распределение на плоскости

#v(1em)

$display(
  f(x, y) = cases(
    1/S&\, a <= x <= b\, c <= y <= d,
    0&\, "вне" a <= x <= b\, c <= y <= d,
  ) \
  F(x, y) = cases(
    1/((b-a) (d-c)) integral_0^x integral_0^y dif x dif y = (x-a)/(b-a) dot (y-c)/(d-c)&\, a <= x <= b\, c <= y <= d,
    0&\, x < a or y < c,
    1&\, x > b\, y > d,
  ) \
  F_1(x) = (x-a)/(b-a)\, a <= x <= b \
  F_2(x) = (y-c)/(d-c)\, c <= y <= d
)$

==== Нормальное (Гауссовское) распределение на плоскости

#v(1em)

$(X, Y)$
$
  f(x, y) = 1/(2pi sigma(X) sigma(Y) sqrt(1 - r^2(X, Y))) exp(-1/(2(1 - r^2(x, y))) (x - M(X))^2/(sigma^2(X)) - 2r(X, Y) times \ times (x - M(X))/(sigma(X)) dot (y - M(y))/(sigma(Y)) + (y - M(Y))^2/(sigma^2(Y)))
$
$display(
  k = mat(D(X), k(X, Y); k(X, Y), D(Y)) = mat(sigma^2(X), r(X, Y) sigma(X) sigma(Y); r(X, Y) sigma(X) sigma(Y), sigma^2(Y)) \
  abs(k) = sigma^2(X) sigma^2(Y) (1 - r^2(X, Y)) \
  M(X) = integral_(-oo)^(+oo) integral_(-oo)^(+oo) x f(x, y) dif x dif y \
  M(Y) = integral_(-oo)^(+oo) integral_(-oo)^(+oo) y f(x, y) dif x dif y \
  k(X, Y) = integral_(-oo)^(+oo) integral_(-oo)^(+oo) (x - M(X)) (y - M(Y)) \
  f_1(x) = integral_(-oo)^(+oo) f(x, y) dif y = 1/(sigma(X) sqrt(2pi)) e^(-(x - M(X))^2/(2sigma^2(X))) \
  f_2(x) = integral_(-oo)^(+oo) f(x, y) dif x = 1/(sigma(Y) sqrt(2pi)) e^(-(y - M(Y))^2/(2sigma^2(Y))) \
  f(x, y) = 1/(2pi sigma(X) sigma(Y)) exp(-1/2 dot ((x - M(X))^2/(sigma^2(X)) + (y - M(Y))^2/(sigma^2(Y)))) = \ = 1/(sigma(X) sqrt(2pi)) e^(-(x - M(X))^2/(2sigma^2(X))) dot 1/(sigma(Y) sqrt(2pi)) e^(-(y - M(Y))^2/(2sigma^2(Y))) = f_1(x) f_2(y) \
  f(x\/y) = 1/(sigma(X) sqrt(1 - r^2(X, Y)) sqrt(2pi)) exp(-1/(2(1 - r^2(X, Y))) ((x - M(X))/(sigma(X)) - r(X, Y) (y - M(Y))/(sigma(Y)))^2) = \ = 1/(sqrt(2pi) sigma(X\/y)) e^(-(x - M(X\/y))^2/(2sigma^2(X\/y)))
)$

$display(
  f(y\/x) = 1/(sigma(Y) sqrt(1 - r^2(X, Y)) sqrt(2pi)) exp(-1/(2(1 - r^2(X, Y))) ((y - M(Y))/(sigma(Y)) - r(X, Y) (x - M(X))/(sigma(X)))^2) = \ = 1/(sqrt(2pi) sigma(Y\/x)) e^(-(y - M(Y\/x))^2/(2sigma^2(Y\/x))) \
  M(X\/y) equiv x = M(X) + r(X, Y) (sigma(X))/(sigma(Y)) (y - M(Y)) \
  M(X\/y) equiv y = M(Y) + r(X, Y) (sigma(Y))/(sigma(X)) (x - M(X)) \
  D(X\/y) = sigma^2(X) (1 - r^2(X, Y))\, space sigma(X\/y) = sigma(X) sqrt(1 - r^2(X, Y)) \
  D(Y\/x) = sigma^2(Y) (1 - r^2(X, Y))\, space sigma(Y\/x) = sigma(Y) sqrt(1 - r^2(X, Y)) \
  f(M(X), M(Y)) = 1/(2pi sigma(X) sigma(Y) (1 - r^2(X, Y))) \
  (x - M(X))^2/(sigma^2(X)) - 2r(X, Y) dot (x - M(X))/(sigma(X)) dot (y - M(y))/(sigma(Y)) + (y - M(Y))^2/(sigma^2(Y)) = a^2 \
  a^2 = -2(1 - r^2(X, Y)) ln(2pi c sigma(X) sigma(Y) sqrt(1 - r^2(X, Y))) \
  0 < c < f(M(X), M(Y)) \
  alpha = 1/2 "arctg" (2"cov"(X, Y))/(sigma^2(X) - sigma^2(Y)) \
  X_1 = (X - M(X)) cos alpha + (Y - M(Y)) sin alpha \
  Y_1 = -(X - M(X)) sin alpha + (Y - M(Y)) cos alpha \
  f(x_1, y_1) = 1/(2pi sigma(X_1) sigma(Y_1)) exp(-1/2 (x_1/(sigma(X_1)))^2 - 1/2 (y_1/(sigma(Y_1)))^2) \
  sigma(X_1) = sqrt(sigma^2(X) cos^2 alpha + "cov"(X,Y) sin 2alpha + sigma^2(Y) sin^2 alpha) \
  sigma(Y_1) = sqrt(sigma^2(X) sin^2 alpha - "cov"(X,Y) sin 2alpha + sigma^2(Y) cos^2 alpha) \
  E(X_1) = rho sqrt(2) sigma(X_1) \
  E(Y_1) = rho sqrt(2) sigma(Y_1) \
  rho = "0,4769" quad rho sqrt(2) = "0,6744" \
  sigma(X_1) = "1,483"E(X_1) \
  sigma(Y_1) = "1,483"E(Y_1)
)$

#line(length: 100%)

$display(
  M(X)\, M(Y) \
  cases(
    delim: "|",
    X\, Y "- НЗСВ (НКСВ)",
    r(X, Y) = 0,
  ) \
  sigma(X) = sigma(Y) = sigma
)$
$
  rect P(x_1 < X < x_2, y_1 < Y < y_2) = (Phi((x_2 - M(X))/(sigma(X))) - Phi((x_1 - M(X))/(sigma(X)))) times \ times (Phi((y_2 - M(Y))/(sigma(Y))) - Phi((y_1 - M(Y))/(sigma(Y))))
$
$
  ellipse P((X, Y) in D_э) = 1 - e^(-rho^2 k^2) quad k^2 = x^2/(sigma^2(X)) + y^2/(sigma^2(Y))
$
$"а)" k = 1, P((X, Y) in D_э_1) = 1 - e^(-rho^2) approx "0,203" quad E_R = "1,75"E \
"б)" k = 4, P((X, Y) in D_э_4) = 1 - e^(-16rho^2) approx "0,974"$
$
  circle k = R/E, P((X, Y) in D_"кр") = 1 - e^(-rho^2), E = rho sqrt(2) sigma = "0,6744"sigma
$

Для нормально распределенных СВ независимость и некоррелированность являются эквивалентными понятиями.

= Случайные последовательности. Предельные теоремы теории вероятностей

#v(1em)

Делятся на законы больших чисел (ЗБЧ) и центральную предельную теорему (ЦПТ). Являются основой для математической статистики. Устанавливают зависимость между случайностью и необходимостью. Дают возможность не только осуществлять научные прогнозы, но и оценивать их точность.

ЗБЧ - группа теорем о предельных значениях случайных величин (Чебышёв, Марков, Бернулли, Пуассон).

ЦПТ - группа теорем, которая говорит о сходимости ЗР к нормальному (Лев$acute(и)$, Ляпунов, ЛТ и ИТ по Муавру-Лапласу).

== ЗБЧ

#v(1em)

Последовательность $x_1, x_2, ..., x_n$ сходятся по вероятности к неслучайной величине $ x_n stretch(->)_(n->oo)^P X "если" forall epsilon > 0: lim_(n->oo) P(abs(X_n - X) < epsilon) = 1 $
Среднеквадратическая сходимость $ X_n stretch(->)_(n->oo)^"С.К." X "если" lim_(n->oo) M(abs(X_n - X)^2) = 0 $
Сходимость почти наверное $ X_n stretch(->)_(n->oo)^"п.н." X "если" P(omega in Omega: lim_(n->oo) X_n (omega) - X_omega) = 0 $

_Далее была приведена теорема Гливенко-Кантелли в неясной формулировке..._

=== Неравенства Маркова и Чебышёва

#v(1em)

Если возможные значения СВ неотрицательны, и существует M(X), то

$forall epsilon > 0 "если" exists M(X)$

Первое неравенство Маркова $ P(X >= epsilon) <= M(X)/epsilon $
Второе неравенство Маркова $ P(X < epsilon) >= 1 - M(X)/epsilon $

#grid(
  columns: (1fr, 1.5fr),
  column-gutter: 1em,
  align: center + horizon,
  [
    #table(
      columns: 7,
      align: center,
      [$x_i$], [1], [2], [3], [5], [7], [8],
      [$p_i$], [0,1], [0,3], [0,2], [0,1], [0,2], [0,1],
    )
  ],
  [
    $P(X <= 7)& = "0,9" \
    P(X <= 7)& > 1 - 4/7 = 3/7 approx "0,429"$
  ],
)

$forall epsilon > 0 "если" exists M(X), D(X)$

Первое неравенство Чебышёва $ P(abs(X - M(X)) >= epsilon) <= D(X)/epsilon^2 $
Второе неравенство Чебышёва $ P(abs(X - M(X)) < epsilon) >= 1 - D(X)/epsilon^2 $
В обоих случаях $epsilon >= sigma(X)$

В нормальном ЗР ($X ~ N(m, sigma^2)$) $ P(abs(X - M(X)) < epsilon) = 2Phi(epsilon/sigma(X)) $

==== Примеры

#v(1em)

#grid(
  columns: (1fr, 2fr),
  column-gutter: 1em,
  align: horizon,
  [
    $n = 10 \
    p = "0,05" \
    "Найти" P(X < 2) "и" P(X >= 2)$
  ],
  [
    $M(X) = n p = "0,5", space D(X) = n p q = "0,475" \
    P(abs(X - M(X)) < 2) >= 1 - "0,475"/2^2 approx "0,881" \
    P(abs(X - "0,5") >= 2) <= "0,475"/2^2 approx "0,119"$
  ],
)

Производительность фермы в среднем 1000 литров молока

$M(X) = 1000 "л", sigma(X) = 200 "л" \
"Вероятность того, что ферма произведёт меньшее 2000 литров - ?"$
+ $P(X <= 2000) >= 1 - 1000/2000 = "0,5"$

+ $P(X <= 2000) = P(0 <= X <= 2000) = P(abs(X - 1000) < 1000) >= 1 - 200^2/1000^2 = "0,96"$

=== Теоремы Чебышёва и Маркова

#v(1em)

$x_1, x_2, ..., x_n "- ЗСВ" quad display(macron(x) = 1/n sum_(i=1)^n x_i stretch(->)_(n->oo)^P 1/n sum_(i=1)^n M(x_i)) \
exists M(x_i), D(x_i) <= D$
$
  & cases(
      delim: #none,
      display(lim P(abs(1/n sum_(i=1)^n x_i - 1/n sum_(i=1)^n M(x_i)) < epsilon) = 1),
      display(lim P(abs(1/n sum_(i=1)^n x_i - 1/n sum_(i=1)^n M(x_i)) >= epsilon) = 0),
    ) quad "ЗБЧ в форме Чебышёва" \
  & lim P(abs(1/n sum_(i=1)^n x_i - 1/n sum_(i=1)^n M(x_i)) >= epsilon) <= D/(n epsilon^2) space "теорема Чебышёва"
$

В теореме Маркова фигурируют независимые СВ

$x_1, x_2, ..., x_n "- НЗСВ" quad display(macron(x) = 1/n sum_(i=1)^n x_i stretch(->)_(n->oo)^P 1/n sum_(i=1)^n M(x_i)) \
exists M(x_i), D(x_i) <= D, abs(abs(k_(i j))) = abs(abs(M(limits(x_i)^(#circle(radius: 1pt, stroke: 0.4pt)), limits(x_j)^(#circle(radius: 1pt, stroke: 0.4pt))))) space i, j = ov(1\,n), M(macron(X)), \
D(macron(X)) = display(1/n^2 sum_(i=1)^n sum_(j=1)^n k_(i j)\, lim D(macron(X)) = 0)$
$
  lim P(abs(1/n sum_(i=1)^n x_i - 1/n sum_(i=1)^n M(x_i)) >= epsilon) <= D(macron(X))/(n epsilon^2) space "теорема Маркова"
$

=== Теоремы Бернулли и Пуассона

#v(1em)

#grid(
  columns: (1fr, 1fr),
  column-gutter: 1em,
  [
    $display(
      n\, m \
      n/m stretch(->)_(n->oo)^P P \
      lim_(n->oo) P(abs(m/n - P) < epsilon) = 1 \
      P(abs(m/n - P) >= epsilon) <= (p q)/(n epsilon^2)
    )$
  ],
  [
    $display(
      n\, m \
      p_i quad n/m stretch(->)_(n->oo)^P 1/n sum_(i=1)^n p_i \
      lim_(n->oo) P(abs(m/n - 1/n sum_(i=1)^n p_i) < epsilon) = 1 \
      P(abs(m/n - 1/n sum_(i=1)^n p_i) >= epsilon) <= (display(sum_(i=1)^n) p_i q_i)/(n^2 epsilon^2)
    )$
  ],
)

$n = 500 \
n_1 = 200 quad p_1 = "0,4" \
n_2 = 180 quad p_2 = "0,5" \
n_3 = 120 quad p_3 = "0,6" \
P(abs(m/n - 1/n sum ...) <= "0,05") >= (sum ...)/() \
x_1, x_2, ..., x_n "- НЗСВ"$

== ЦПТ

#v(1em)

Группа теорем о сходимости каких-то сумм к нормальному закону распределения

=== Теорема Леви

#v(1em)

$X_1, X_2, ..., X_n "- НЗСВ" \
X_i ~ "ЗР", exists D(X_i) = sigma^2$
$
  &S_n^* = tilde(X) = display(sum_(i=1)^n X_i - sum_(i=1)^n M(X_i))/display(sqrt(sum_(i=1)^n D(X_i))) = display(sum_(i=1)^n X_i - n a)/sqrt(n sigma^2) = display(1/n sum_(i=1)^n X_i - a)/display(sigma/sqrt(n)) = (macron(X) - a)/display(sigma/sqrt(n)) \
  &S_n^* stretch(->)_(n->oo)^P N(0; 1)
$

=== Теорема Ляпунова

#v(1em)

$display(
  z_n = display(sum_(i=1)^n (X_i - M(X_i)))/display(sqrt(sum_(i=1)^n D(X_i))) \
  forall epsilon > 0 quad lim_(n->oo) 1/display(sum_(i=1)^n D(X_i)) sum_(i=1)^n integral_a (x - M(X))^2 f_i (x) dif x \
  a = abs(x - M(X_i)) > epsilon sqrt(sum_(i=1)^n D(X_i)) \
  F(z_n) = P(z_n < x) stretch(->)_(n->oo)^P 1/sqrt(2pi) integral_(-oo)^x e^(-t^2/2) dif t
)$

$display(
  X_1\, X_2\, ...\, X_n "- НЗСВ" \
  X_i ~ "ЗР" quad exists M(X_i) = a\, D(X_i) = sigma^2 \
  z_n = display(sum_(i=1)^n (x_i - a))/(sigma sqrt(n)) quad F(z_n) = P(z_n < x) stretch(->)_(n->oo)^P 1/sqrt(2pi) integral_(-oo)^x e^(-t^2/2) dif t \
  S_n = sum_(i=1)^n X_i
)$
$
  P(s_1 < S < s_2) = Phi((s_2 - n a)/(sigma sqrt(n))) - Phi((s_1 - n a)/(sigma sqrt(n)))
$
$
  cases(
    reverse: #true,
    X_1\, X_2\, ...\, X_n "- НЗСВ",
    exists M(X_i) = a_i\, D(X_i) = sigma_i^2,
    mu_3(X_i) = M(abs(X_i - a_i)^3),
    display(lim_(n->oo)) display(sum_(i=1)^n mu_3(X_i))/(display(sum_(i=1)^n sigma_i^2))^(3\/2) = ...
  )
  "ЗР" S_n = sum_(i=1)^n X_i stretch(->)_(n->oo)^P N(sum_(i=1)^n a_i; sum_(i=1)^n sigma_i^2) \
  lim P(abs(display(S_n - sum_(i=1)^n a_i)/display(sqrt(sum_(i=1)^n sigma_i^2))) < z) = 1/sqrt(2pi) integral_(-oo)^z e^(-t^2/2) dif t
$
$
  cases(
    reverse: #true,
    X_1\, X_2\, ...\, X_n "- НЗСВ",
    exists M(X_i) = a\, D(X_i) = sigma^2\, M(abs(X_i - a_i)^3) = mu_3(X_i)
  ) Y_n = sum_(i=1)^n X_i stretch(->)_(n->oo)^P N(0; 1)
$
#grid(
  columns: (1fr, 3fr),
  column-gutter: 1em,
  [
    $n = 500 \
    (M(x) - 8)"г" \
    sigma(X) = 40"г" \
    M(X_i) = a \
    sigma(X_i) = sigma = 40"г"$
  ],
  [
    $P(macron(X) <= a - 8) = P(-oo < macron(X) <= a - 8) = \ = P display(((-oo - a)/(sigma sqrt(n)) n < (macron(X) - a)/(sigma sqrt(n)) < ((a - 8) - a)/(sigma sqrt(n)) n)) = Phi(display(-8/sigma) sqrt(n)) - Phi(-oo) = \ = Phi(+oo) - Phi(display((8 sqrt(500))/40)) = "0,5" - "0,499997" approx "0,000003г"$
  ],
)

=== Локальная и интегральная теоремы Муавра-Лапласа

#v(1em)

$ P_n (m) approx f(x)/sqrt(n p q), $ $display(f(x) = 1/sqrt(2pi) e^(-x^2/2))$, $display(x = (m - n p)/sqrt(n p q))$

==== Локальная теорема

#v(1em)

Применяется в случае $n p > 10$
$
  lim_(n->oo) (P_n (m))/(1/sqrt(2pi n p q) e^(-x^2/2)) = 1
$

==== Интегральная теорема

#v(1em)

$
  P_n (m_1 < m < m_2) approx Phi underparen(((m_2 - n p)/sqrt(n p q)), x_2) - Phi underparen(((m_1 - n p)/sqrt(n p q)), x_1)
$

#grid(
  columns: (1fr, 1fr),
  column-gutter: 1em,
  [
    + $f(-x) = f(x)$
    + $f(x) > 0$
    + $display(lim_(x->+oo)) f(x) = 1$
    + $display(lim_(x->-oo)) f(x) = 0$
  ],
  [
    + $Phi(-x) = -Phi(x)$
    + $Phi(x_2) > Phi(x_1), x_2 > x_1$
    + $Phi(0) = 0$
    + $display(lim_(x->oo)) Phi(x) = 1$
  ],
)

==== Примеры

#v(1em)

#grid(
  columns: (1fr, 2.5fr),
  column-gutter: 1em,
  [
    $p = "0,6" \
    n = 100 => n p = 60 \
    m = 55 => n p q = 24 \
    x = (m - n p)/sqrt(n p q) = (55 - 60)/sqrt(24) approx "-1,02"$
  ],
  [
    $P_100(55) = C_100^55 ("0,6")^55 ("0,4")^45 = "0,04831" \
    P_100(55) = f("-1,02")/sqrt(24) = "0,2371"/"4,899" = "0,04835"$
  ],
)

#grid(
  columns: (1fr, 2.5fr),
  column-gutter: 1em,
  [
    $n = 1000 quad n p = 600 \
    p = "0,6" quad n p q = 240 \
    m_1 = 580 sqrt(n p q) = "15,49" \
    m_2 = 630$
  ],
  [
    $x_1 = (580 - 600)/"15,49" = "-1,29", x_2 = (630 - 600)/"15,49" = "1,94" \
    P_1000(580 <= m <= 630) approx Phi("1,94") - Phi("-1,29") = \ = "0,4738" - ("-0,4015") = "0,8753"$
  ],
)

#grid(
  columns: (1fr, 1.5fr),
  column-gutter: 1em,
  [
    $n = 10 \
    #table(
      columns: 6,
      align: center,
      [$x_i$], [0], [1], [2], [3], [4],
      [$p_i$], [0,25], [0,3], [0,2], [0,15], [0,1],
    ) \
    M(X_i) = "1,55" \
    M(X) = display(sum_(i=1)^10) M(X_i) = "15,5" \
    D(X_i) = "1,65" \
    sigma(X_i) = "1,28" \
    sigma(X) = display(sqrt(sum_(i=1)^10 sigma^2 (X_i))) = \ = sqrt(10 dot "1,65") = "4,06"$
  ],
  [
    $P(X >= 15) = P(15 <= x < oo) = "0,5" - Phi((15 - "15,5")/"4,06") = \ = "0,5" + "0,0478" = "0,5478" \
    m_min space P_д = "0,93" \
    P_д = "0,5" - Phi((m_min - "15,5")/"4,06") = "0,93" \
    Phi((m_min - "15,5")/"4,06") = "-0,43" \
    -(m_min - "15,5")/"4,06" = "1,475" \
    m_min = "-1,475" dot "4,06" + "15,5" approx "9,5" = 9$
    #line(length: 100%, stroke: 0.5pt)
    $n "- ?" \
    X >= 10 \
    P_д = "0,98" = "0,5" - Phi((10 - "1,55"n)/sqrt("1,65"n)) \
    Phi((10 - "1,55"n)/sqrt("1,65"n)) = "-0,48" \
    -(10 - "1,55"n)/sqrt("1,65"n) = "2,05" => n = "12,433" = 13$
  ],
)
